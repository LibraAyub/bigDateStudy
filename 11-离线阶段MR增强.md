# 11-离线阶段MR增强

## 1.MapTask工作机制

```
是默认由TestInputFormat读取 getSplits。逻辑切片规划得到 splits。
在由 RecordReader 对象调用方法LineRecordReader读取以\n 作为分隔符，读取一行数据，返回  K每行的其实偏移量，v这一行内容。
执行用户的map函数。输出给context 。 
在 collect 中，会先对其进行分区处理，默认使用 HashPartitioner。来决定交由那个reduce task处理。
然后调用OutputCollect收集  再存入环形缓存区（内存，都是kv对） 默认大小100mb  溢出比：0.8 溢出才向磁盘写入。按字段序排序。如果设置了 Combiner，那就是这个时候了。
每次溢写会在磁盘上生成一个临时文件（写之前判断是否有 combiner），如果有多个临时文件就会merge合并，因为最终只有一个写入磁盘。

提示;使用combiner不能影响业务。 比如求中位数。
```

## 2 reduce Task工作机制

```
大致分为copy、sort、reduce(通过Http get去每一个maptask端拉取自己的分区)   设置了JOB.setNumReduceTask（2）就会有两个reduceTask.拉取来了后合并（有三种形式）内存到磁盘，磁盘到磁盘，内存到内存。合并好的已经排序好的。 然后调用wordCountReduce（k,itertor,context）是根据k是否相同为一组。调用我们自己重写的reduce方法。
```

## 3.Shuffle机制

```
map阶段处理的数据如何传递给reduce阶段。这个过程叫shuffle.
shuffle:map阶段产生数据输出开始，到reduce阶段取得数据作为输入之前的过程叫做shuffle.发牌，洗牌（数据分区，排序，合并）
Collect 阶段：将 MapTask 的结果输出到默认大小为 100M 的环形缓冲区，
保存的是 key/value，Partition 分区信息等。
Spill 阶段：当内存中的数据量达到一定的阀值的时候，就会将数据写入本地磁盘，
Merge 阶段：吧所有的溢出文件合并。
Copy 阶段：ReduceTask 启动 Fetcher 线程到已经完成 MapTask 的节点上复制一份属于自己的数据、。
Sort 阶段：在对数据进行合并的同时，会进行排序操作。
```

## 4 Maptask并行度机制

```
MapTask 的并行度指的是 map 阶段有多少个并行的 task 共同处理任务。
一个mapreduce的map阶段并行度由客户端在提交job时决定。即逻辑切片。切片有getsplit(),job提交集群之前就应该完成好切片。针对小文件的存储场景：建议在上传到hdfs之前进行合并然后在上传。避免影响后续执行效率
```

## 5 reduce并行度机制

```
reducetask 并行度同样影响整个 job 的执行并发度和执行效率，与 maptask的并发数由切片数决定不同，Reducetask 数量的决定是可以直接手动设置：
	job.setNumReduceTasks(4);
数据分布均匀不？数据倾斜
输入改变了reducetask   考虑默认分区是否符合你的要求；
默认hash取模，如果不满意，需要自定义分区注解paartitioner
如果需求中有全局这样的字眼，意味着最终只有一个输出文件，也就是一个reducetask.
```

## 6.优化参数

###  6.1 资源参数

```
mapreduce.map.memory.mb: 默认1024MB
mapreduce.reduce.memory.mb: 默认1024MB
mapreduce.map.cpu.vcores: 每个Maptask cpu数目默认1
mapreduce.reduce.cpu.vcores: 每个reduce task cpu数目默认1.

//shuffle的默认设置  
mapreduce.task.io.sort.mb 100 shuffle 的环形缓冲区大小，默认 100m
mapreduce.map.sort.spill.percent 0.8 环形缓冲区溢出的阈值，默认 80%
```

### 6.2 容错参数

```
mapreduce.map.maxattempts: 每个 Map Task 最大重试次数，一旦重试参数超过该值，则认为 Map Task 运行失败，默认值：4。

```

### 6.3 效率和稳定

```
mapreduce.map.speculative: 是否为 Map Task 打开推测执行机制，默认为 true, 如果为 true，则可以并行执行一些 Map 任务的多个实例。

```

### 6.4案列

```
正排索引： 文件---》内容 的映射
	1.html-->  hello  apple  hadoop
	创建简单；但是不利于搜索。
	
倒排索引：内容--》文件  的映射
	有利于我们搜索，实际应用中还可以加上权重。权重大的优先推荐。商业公司还可以加上竞价排名。
```

## 7 YARN

### 7.1定义

```
它是一个通用资源管理系统和调度平台，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。
```

### 7.2 特点

```
yarn并不清楚用户提交的程序的运行机制；yarn只提供运算资源的调度；yarn中主管角色叫ResourceManager;yarn 上可以运行各种类型的分布式运算程序;提高资源利用率，方便数据共享。
```

###  7.3 三大组件

```
ResourceManager:负责整个集群的资源管理和分配，是一个全局的资源管理系统。
NodeManager:是每个节点上的资源和任务管理器，他是管理这台机器的代理负责该节点程序的运行，以及该节点资源的监控。定时向 ResourceManager 汇报本节点资源。
ApplicationMaster:用户提交的都有一个master;负责与Rm调度协商获取资源；监控所有任务运行。
对于所有的 applications，RM 拥有绝对的控制权和对资源的分配权。而每个 AM 则会和RM 协商资源，同时和 NodeManager 通信来执行和监控 task。
```

### 7.4 yarn运行流程

```
1.client 向 RM 提交应用程序；2. ResourceManager 启动一个container（容器） 用于运行 ApplicationMaster；3. ApplicationMaster 向 ResourceManager 注册自己保持连接。4.指定位置读取切片规划； 5.根据切片申请相应数量的资源；6.RM收到申请， 通过nm启动RmAppmaster容器位置；7.MRApp在申请到的容器启动mapTask; 
8.container 运行期间，ApplicationMaster 对 container 进行监控。container 通过 RPC协议向对应的 AM 汇报自己的进度和状态等信息.
9.应用运行期间，client 直接与 AM 通信获取应用的状态、进度更新等信息。
10.应用运行结束后，ApplicationMaster 向 ResourceManager 注销自己，并允许属于它的container 被收回。
```

### 7.5 yarn的调度器Scheduler

```
在 Yarn 中有三种调度器可以选择：FIFO Scheduler ，Capacity Scheduler，Fair
Scheduler。
```

#### 	7.5.1 FIFO Scheduler

```
先进先出调度器；FIFO Scheduler 是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的应用占用资源多。
```

#### 	7.5.2 Capacity （容量）Scheduler

```
Capacity 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。
```

#### 	7.5.3  Fair Scheduler（公平调度器）

```
我们不需要预先占用一定的系统资源，Fair 调度器会为所有运行的job 动态的调整系统资源。需要注意的是，在下图 Fair 调度器中，从第二个任务提交到获得资源会有一定的延迟。
```